{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from preprocessing import Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jacopo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download stopwords dataset\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                 int64\n",
       "PostTypeId         int64\n",
       "CreationDate      object\n",
       "Score              int64\n",
       "ViewCount        float64\n",
       "FavoriteCount    float64\n",
       "Title             object\n",
       "Body              object\n",
       "Tags              object\n",
       "Topic             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/stackechange_csv/datascience.stackexchange.com-posts.csv', sep=',')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Body'].fillna('', inplace=True)\n",
    "df['Title'].fillna('', inplace=True)\n",
    "df['Tags'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_html(column: str):\n",
    "#   df[column] = df[column].str.replace('<.{1,6}>', '')\n",
    "\n",
    "# def apply_lowercase(column: str):\n",
    "#   df[column] = df[column].str.lower()\n",
    "\n",
    "# def remove_special_characters(column: str):\n",
    "#   df[column] = df[column].str.replace('\\W', ' ')\n",
    "\n",
    "# def apply_decontractions(column: str):\n",
    "#   df[column] = df[column].str.replace(\"won't\", \"will not\").str.replace(\"can\\'t\", \"can not\").str.replace(\"n\\'t\", \" not\").str.replace(\"\\'re\", \" are\").str.\\\n",
    "#                                                 replace(\"\\'s\", \" is\").str.replace(\"\\'d\", \" would\").str.replace(\"\\'ll\", \" will\").str.\\\n",
    "#                                                 replace(\"\\'t\", \" not\").str.replace(\"\\'ve\", \" have\").str.replace(\"\\'m\", \" am\")\n",
    "\n",
    "# def preprocess_body(column):\n",
    "#   remove_html(column)\n",
    "#   apply_lowercase(column)\n",
    "#   remove_special_characters(column)\n",
    "\n",
    "\n",
    "# def preprocess_title(column):\n",
    "#   remove_special_characters(column)\n",
    "#   apply_lowercase(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        how can i do simple machine learning without h...\n",
       "1        what open source books  or other materials  pr...\n",
       "2                                                      nan\n",
       "3                                                      nan\n",
       "4                 is data science the same as data mining \n",
       "                               ...                        \n",
       "72866    question about non linearity of activation fun...\n",
       "72867                                                  nan\n",
       "72868    is it possible to  link couple connect  certai...\n",
       "72869                                                  nan\n",
       "72870                      one word changes everything nlp\n",
       "Name: Title, Length: 72871, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Title'] = df['Title'].apply(Preprocessing.apply_lowercase)\n",
    "df['Title'] = df['Title'].apply(Preprocessing.remove_special_characters)\n",
    "df['Title'] = df['Title'].apply(Preprocessing.remove_html)\n",
    "\n",
    "df['Title']\n",
    "\n",
    "# TODO: try gensim simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'open',\n",
       " 'source',\n",
       " 'books',\n",
       " 'or',\n",
       " 'other',\n",
       " 'materials',\n",
       " 'provide',\n",
       " 'a',\n",
       " 'relatively',\n",
       " 'thorough',\n",
       " 'overview',\n",
       " 'of',\n",
       " 'data',\n",
       " 'science']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "sentences = []\n",
    "for title in df['Title'].values:\n",
    "    words = nltk.word_tokenize(title)\n",
    "    sentences.append(words)\n",
    "\n",
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['open',\n",
       " 'source',\n",
       " 'books',\n",
       " 'materials',\n",
       " 'provide',\n",
       " 'relatively',\n",
       " 'thorough',\n",
       " 'overview',\n",
       " 'data',\n",
       " 'science']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stopwords from vocabulary\n",
    "from nltk.corpus import stopwords\n",
    "# stopwords.words('english')\n",
    "def remove_stopwords(words):\n",
    "    return [w for w in words if w not in stopwords.words('english')]\n",
    "\n",
    "sentences = [remove_stopwords(words) for words in sentences]\n",
    "    \n",
    "sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between query (2nd document) and 3rd document: 0.0%\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, similarities\n",
    "from gensim.models import TfidfModel\n",
    "corpus = sentences\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "# dictionary.token2id\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "tfidf = TfidfModel(bow_corpus)\n",
    "\n",
    "# transform the whole corpus via TfIdf and store in index matrix\n",
    "nf=len(dictionary.dfs)\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=nf)\n",
    "\n",
    "query = corpus[1]\n",
    "query_bow = dictionary.doc2bow(query)\n",
    "\n",
    "# Compute similarity between query and this index\n",
    "sims = index[tfidf[query_bow]]\n",
    "print(f'Similarity between query (2nd document) and 3rd document: {sims[2]}%')\n",
    "# Similarity between query and each document sorted\n",
    "res = [e for e in sorted(enumerate(sims), key=lambda x: x[1], reverse=True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['term', 'r', 'squared', 'vif', 'variance', 'inflation', 'factor', 'different', 'normal', 'r', 'squared', 'calculation'], 0.9804725), (['get', 'p', 'value', 'confident', 'interval', 'logisticregression', 'sklearn'], 0.97957623), (['regression', 'scatterplot', 'low', 'r', 'squared', 'high', 'p', 'values'], 0.97941035), (['difference', 'r', 'squared', 'adjusted', 'r', 'squared'], 0.97894543), (['r', 'phi', 'coefficient', 'calculation'], 0.9758071), (['necessary', 'take', 'log', 'transformation', 'data', 'values', 'get', 'minimum', 'mean', 'squared', 'error'], 0.9755108), (['computing', 'adjusted', 'p', 'values', 'batches'], 0.97486234), (['least', 'mean', 'square', 'linear', 'regression', 'discrete', 'values', 'axis'], 0.9743969), (['getting', 'wrong', 'ch2', 'values', 'sklearn', 'chi2'], 0.9724077), (['getting', 'different', 'chi', 'square', 'values', 'sklearn', 'function'], 0.9722057)]\n"
     ]
    }
   ],
   "source": [
    "from embeddings import Embeddings\n",
    "\n",
    "Embeddings.word2vec_similarity(\"hadoop\", corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacopo\\AppData\\Local\\Temp\\ipykernel_23972\\3307185406.py:11: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  word2vec.init_sims(replace=True)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Key 'classification' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [13], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39m# Precompute L2-normalized vectors. \u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m# If replace is set, forget the original vectors and only keep the normalized ones = saves lots of memory!\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m# Note that you cannot continue training after doing a replace. The model becomes effectively read-only = you can call most_similar, similarity etc., but not train.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m word2vec\u001b[39m.\u001b[39minit_sims(replace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 12\u001b[0m word2vec\u001b[39m.\u001b[39;49mwv\u001b[39m.\u001b[39;49mmost_similar(\u001b[39m'\u001b[39;49m\u001b[39mclassification\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Jacopo\\Documents\\Projects\\ml-search-engine\\preprocessing\\venv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:842\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    839\u001b[0m         weight[idx] \u001b[39m=\u001b[39m item[\u001b[39m1\u001b[39m]\n\u001b[0;32m    841\u001b[0m \u001b[39m# compute the weighted average of all keys\u001b[39;00m\n\u001b[1;32m--> 842\u001b[0m mean \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_mean_vector(keys, weight, pre_normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, post_normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, ignore_missing\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    843\u001b[0m all_keys \u001b[39m=\u001b[39m [\n\u001b[0;32m    844\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_index(key) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m keys \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, _KEY_TYPES) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_index_for(key)\n\u001b[0;32m    845\u001b[0m ]\n\u001b[0;32m    847\u001b[0m \u001b[39mif\u001b[39;00m indexer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(topn, \u001b[39mint\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Jacopo\\Documents\\Projects\\ml-search-engine\\preprocessing\\venv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:519\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[1;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[0;32m    517\u001b[0m         total_weight \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mabs\u001b[39m(weights[idx])\n\u001b[0;32m    518\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m ignore_missing:\n\u001b[1;32m--> 519\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKey \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not present in vocabulary\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    521\u001b[0m \u001b[39mif\u001b[39;00m(total_weight \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m    522\u001b[0m     mean \u001b[39m=\u001b[39m mean \u001b[39m/\u001b[39m total_weight\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'classification' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "# Word2Vec model\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "\n",
    "# Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "word2vec = Word2Vec(corpus, min_count=10, sg=0, window=10)\n",
    "\n",
    "# Precompute L2-normalized vectors. \n",
    "# If replace is set, forget the original vectors and only keep the normalized ones = saves lots of memory!\n",
    "# Note that you cannot continue training after doing a replace. The model becomes effectively read-only = you can call most_similar, similarity etc., but not train.\n",
    "word2vec.init_sims(replace=True)\n",
    "word2vec.wv.most_similar('classification')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beer\n",
      "[(['citra', 'hop', 'differ', 'hops'], 0.0), (['first', 'beer', 'ever', 'brewed'], 0.0), (['reduced', 'alcoholic', 'beer', 'made'], 0.0), (['temperature', 'serve', 'beer'], 0.0), (['best', 'angle', 'store', 'beer', 'bottles'], 0.0), (['certain', 'types', 'beer', 'get', 'drunk', 'quickly'], 0.0), (['difference', 'ale', 'lager'], 0.0), (['mull', 'beer'], 0.0), (['ipas', 'cause', 'worse', 'hangovers'], 0.0), (['average', 'brewing', 'time', 'craft', 'beer'], 0.0)]\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec similarity\n",
    "def word2vec_similarity(model, ws1, ws2):\n",
    "    \"\"\"Compute cosine similarity between two sets of words.\"\"\"\n",
    "    return model.wv.n_similarity(ws1, ws2)\n",
    "\n",
    "query = \"beer\"\n",
    "r = sorted([(d, word2vec_similarity(word2vec, query, d)) for d in corpus if len(d) != 0], key=lambda x: x[1], reverse=True)\n",
    "print(query)\n",
    "print(r[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97a897b907aca9de517e1c4566981d80522cd2a1ee1dc0b9152c2c980090622f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
